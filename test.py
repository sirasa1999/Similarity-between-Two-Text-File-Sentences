# -*- coding: utf-8 -*-
"""Copy of Model Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H6b8qkIGJKBWsgMpTSGOELsbqrpMKw25
"""

#from time import time
  import pandas as pd
  import numpy as np
  from gensim.models import KeyedVectors as KVecs
  import re  # regex for string machting
  import nltk
  from nltk.corpus import stopwords
  import keras.backend as K
  from keras.models import load_model

def exponent_neg_manhattan_distance(left, right):
    ''' Helper function for the similarity estimate of the LSTMs outputs'''
    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))

EMBEDDING_FILE = '/content/drive/MyDrive/minor project/GoogleNews-vectors-negative300.bin'
model=load_model("/content/drive/MyDrive/minor project/model.h5")

def text_to_word_list(text):
    ''' Pre process and convert texts to a list of words '''
    text = str(text)  
    text = text.lower()

    # Clean text with regex subs
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r"\0s", "0", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)

    text = text.split()

    return text

# Prepare embedding
vocabulary = dict()
inverse_vocabulary = ['<unk>']  # placeholder for [0, 0, ....0] embedding
word2vec = KVecs.load_word2vec_format(EMBEDDING_FILE, binary=True) 

try:
  stops = set(stopwords.words('english'))
except:
  nltk.download('stopwords')
  stops = set(stopwords.words('english'))

# with open('/content/drive/MyDrive/minor project/1.txt','r') as file:
#     new_data0 = file.read()
# with open('/content/drive/MyDrive/minor project/2.txt','r') as file:
#     new_data = file.read()

with open('/content/drive/MyDrive/minor project/3.txt','r') as file:
    new_data0 = file.read()
with open('/content/drive/MyDrive/minor project/4.txt','r') as file:
    new_data = file.read()

data0 = []
data = []

for word in text_to_word_list(new_data0):
    # Check for unwanted words
    if word in stops and word not in word2vec.vocab:
        continue  

    if word not in vocabulary:
        # inserts unique words into empty `vocabulary` dictionary
        vocabulary[word] = len(inverse_vocabulary)
        data0.append(len(inverse_vocabulary))  # numerizing words
        inverse_vocabulary.append(word)
    else:
        data0.append(vocabulary[word])
    
for word in text_to_word_list(new_data):
    # Check for unwanted words
    if word in stops and word not in word2vec.vocab:
        continue  

    if word not in vocabulary:
        # inserts unique words into empty `vocabulary` dictionary
        vocabulary[word] = len(inverse_vocabulary)
        data.append(len(inverse_vocabulary))  # numerizing words
        inverse_vocabulary.append(word)
    else:
        data.append(vocabulary[word])

data0_len = len(data0)
data_len = len(data)
if data0_len > data_len:
    diff = data0_len - data_len
    for x in range(diff):
        data.append(0)
elif data_len > data0_len:
    diff = data_len - data0_len
    for x in range(diff):
        data0.append(0)
            
embedding_dim = 300
embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix
embeddings[0] = 0  # So that the padding will be ignored

# Build the embedding matrix
for word, index in vocabulary.items():
    if word in word2vec.vocab:
        embeddings[index] = word2vec.word_vec(word)

data1 = data
data2 = data0
data1 = np.array(data1, dtype=np.float32)
data2 = np.array(data2, dtype=np.float32)
predictions = model.predict([data1, data2])
sum = 0
for prediction in predictions:
    sum += prediction[0]
avg = sum / len(data1)
print("Input sentences in file 1 is: \n")
print(new_data0,"\n")
print("Input sentences in file 2 is: \n")
print(new_data,"\n")
print("OUTPUT: \n")
if avg > 0.5:
    print('The sentences are similar.')
else:
    print('The sentences are not similar.')

from google.colab import drive
drive.mount('/content/drive')